{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de Admisión de Estudiantes con Redes Neuronales usando Keras\n",
    "\n",
    "Este ejercicio está inspirado en el Nanodegree de Inteligencia Artificial de Udacity. El objetivo es predecir si aceptamos a un estudiante a graduate studies (maestría, doctorado, etc) a UCLA basándonos en 3 datos:\n",
    "* GRE Scores (Es un examen estandar en todo Estados Unidos).\n",
    "* GPA Scores (Promedio de la carrera, suele ir de 1 a 4).\n",
    "* Class Rank (El rango de la clase, es de 1 a 4).\n",
    "\n",
    "El dataset lo puedes encontrar en: http://www.ats.ucla.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1. Cargar la información\n",
    "\n",
    "Para poder cargar y manipular la información, vamos a utilizar Pandas y Numpy, dos librerías **muy** comunes en Data Science, Machine Learning y Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Pandas y NumPy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leemos el archivo csv (comma separated values)\n",
    "data = pd.read_csv('students-data.csv')\n",
    "\n",
    "# Imprimimos los primeros 10 resultados\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pregunta 1. ¿Cuál sería(n) los features en este problema? ¿Cómo están relacionados con el valor que queremos predecir?**\n",
    "\n",
    "(Puedes dar click en esta celda y escribir tus respuestas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2. Comprender la información.\n",
    "\n",
    "Una de las maneras más fáciles de entender la información es mediante visualizaciones. Lee los comentarios de la siguiente celda para entender qué está ocurriendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función que hace el plot\n",
    "def plot_points(data):\n",
    "    \n",
    "    # Vamos a cargar los dos features numéricos de este problema: GRE y GPA. Lo cargamos como un arreglo de NumPy.\n",
    "    X = np.array(data[[\"gre\",\"gpa\"]])\n",
    "    \n",
    "    # También cargamos el valor que queremos predecir.\n",
    "    y = np.array(data[\"admit\"])\n",
    "    \n",
    "    # Separamos los features según fue aceptado o rechazado\n",
    "    admitted = X[np.argwhere(y==1)]\n",
    "    rejected = X[np.argwhere(y==0)]\n",
    "    \n",
    "    # Hacemos un scatter plot, donde el color rojo es rechazado y los celestes son aceptados.\n",
    "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n",
    "    plt.xlabel('Test (GRE)')\n",
    "    plt.ylabel('Grades (GPA)')\n",
    "    \n",
    "# Mostramos los puntos\n",
    "plot_points(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no hay una relación tan clara, se puede decir que los estudiantes con mejores promedios y calificaciones en el examen son más probables de ser aceptados. De todos modos, la información no se puede separar tan fácil como en otros problemas que hemos enfrentado. Tal vez si usamos el tercer feature, el Rank, se separe mejor la información. Vamos a hacer 4 gráficas como la anterior, una para cada promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el rank\n",
    "data_rank1 = data[data[\"rank\"]==1]\n",
    "data_rank2 = data[data[\"rank\"]==2]\n",
    "data_rank3 = data[data[\"rank\"]==3]\n",
    "data_rank4 = data[data[\"rank\"]==4]\n",
    "\n",
    "# Mostramos los gráficos para cada uno\n",
    "plot_points(data_rank1)\n",
    "plt.title(\"Rank 1\")\n",
    "plt.show()\n",
    "\n",
    "plot_points(data_rank2)\n",
    "plt.title(\"Rank 2\")\n",
    "plt.show()\n",
    "\n",
    "plot_points(data_rank3)\n",
    "plt.title(\"Rank 3\")\n",
    "plt.show()\n",
    "\n",
    "plot_points(data_rank4)\n",
    "plt.title(\"Rank 4\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se entiende mejor. Mientras el rango tiene un valor más bajo (mejor rango), parece que es más probable que seas aceptado. Vamos a utilizar el rango, entonces, como uno de nuestros features, pero, para poder hacerlo, necesitamos hacer algo llamado one-hot encoding.\n",
    "\n",
    "## Paso 3: Modificamos la información para la red neuronal\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "Pandas ya tiene herramientas para hacer esto, por lo que vamos a aprovecharlas. No te preocupes mucho por el código, intenta enfocarte en lo que está ocurriendo. \n",
    "\n",
    "La función get_dummies va a separar variables categóricas en muchas variables. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data['rank'], prefix='rank')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos cómo conseguir esas columnas, las podemos agregar a nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables for rank\n",
    "one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)\n",
    "one_hot_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero ahora tenemos una columna que no nos interesa, rank. Podemos eliminar esa columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the previous rank column\n",
    "one_hot_data = one_hot_data.drop('rank', axis=1)\n",
    "\n",
    "# Print the first 10 rows of our data\n",
    "one_hot_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalando datos\n",
    "\n",
    "Todo se ve bien, pero tenemos un problema. El GRE parece ir de 200 a 800, mientras que el GPA va de 1 a 4. El rango en el GRE es mucho más grande y esto puede causar problemas. En la siguiente celda vamos a escalar la información para que los features estén entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos la información\n",
    "processed_data = one_hot_data[:]\n",
    "\n",
    "# Escalamos ambas columnas\n",
    "processed_data['gre'] = processed_data['gre']/800\n",
    "processed_data['gpa'] = processed_data['gpa']/4.0\n",
    "processed_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar los datos en Training y Testing Set\n",
    "\n",
    "** Pregunta 2. ¿Por qué tenemos que separar en Training y Testing Set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda separaremos en training y testing set. El testing set será el 10% de nuestra info total. No se preocupen mucho por el código, pero entiendan qué está pasando. Si corres la siguiente celda muchas veces, los resultados serán diferentes. Lo que estamos haciendo es agarrar el 90% de los datos utilizando ```np.random.choice```. Especificamos que no usamos remplazo (cuando sacamos un valor, no lo ponemos de vuelta). En la segunda línea lo que hacemos es agarrar los elementos y soltarlos (para el training y para el testing, respectivamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)\n",
    "train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample)\n",
    "\n",
    "print(\"El número de training samples es\", len(train_data))\n",
    "print(\"El número de testing samples es\", len(test_data))\n",
    "print(train_data[:7])\n",
    "print(test_data[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar data en features y labels\n",
    "\n",
    "**Pregunta 3. ¿Cuál es la diferencia entre features y labels?**\n",
    "\n",
    "Si te sale un error en la siguiente celda asegúrate de tener Keras instalado en tu ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# Separar data y hacer one-hot encoding de la salida (aceptado o rechazado)\n",
    "# Nota: Adicionalmente, estamos convirtiendo todo a np.array para que Keras lo pueda utilizar.\n",
    "\n",
    "# Soltamos la columna de admit en los features\n",
    "features = np.array(train_data.drop('admit', axis=1))\n",
    "\n",
    "# Primero convertimos admit en dos columnas (one-hot encoding) usando la función to_categorical de Keras. \n",
    "# Esta es la que se usa normalmente para hacer one-hot encoding.\n",
    "targets = np.array(keras.utils.to_categorical(train_data['admit'], 2))\n",
    "\n",
    "# Hacemos lo mismo para el testing set\n",
    "features_test = np.array(test_data.drop('admit', axis=1))\n",
    "targets_test = np.array(keras.utils.to_categorical(test_data['admit'], 2))\n",
    "\n",
    "print(features[:5])\n",
    "print(targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4. Construimos Arquitectura del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Construimos el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='sigmoid', input_shape=(6,)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(loss = 'mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5. Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, targets, epochs=200, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6. Evaluamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(features, targets)\n",
    "print(\"\\n Training Accuracy:\", score[1])\n",
    "score = model.evaluate(features_test, targets_test)\n",
    "print(\"\\n Testing Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reto. Juega con los parámetros\n",
    "\n",
    "Como puedes ver, hemos hecho muchas decisiones para entrenar nuestro modelo. Por ejemplo: número de capas, tamaño de capas, número de epochs y más. Ahora te toca a ti cambiar los paráemtros y ver si puedes mejorar la precisión en el testing set. Unas sugerencias:\n",
    "\n",
    "* Funciones de activación: Cambia Sigmoid a ReLu. \n",
    "* Función de pérdida: categorical_crossentropy, mean_squared_error\n",
    "* Optimizers: sgd, rmsprop, adam, ada.\n",
    "* Agrega o reduce Dropouts.\n",
    "\n",
    "Es probable que el accuracy no cambie. Esto pasa por tener pocos datos, se encuentra el mismo mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

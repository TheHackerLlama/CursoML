{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Deep Learning\n",
    "\n",
    "En este taller se dará una introducción a cómo funcionan las redes neuronales. Utilizaremos ```Keras```, una librería que hace fácil manipular redes neuronales utilizando Tensorflow, la librería más utilizada para Machine Learning.\n",
    "\n",
    "Corre ```conda install keras``` para instalar Keras.\n",
    "\n",
    "Implementaremos un multi-layer perceptron (red neuronal) que pueda identificar dígitos escritos a mano. Dado que es un problema muy común, ya existe un dataset de 60,000 imágenes de dígitos, llamada [MNIST](http://yann.lecun.com/exdb/mnist/). Este es un excelente caso para aprender a utilizar redes neuronales en casos reales. [Keras](https://keras.io/) funciona como un API (o librería) de nivel más alto. Esto hará más fácil y rápido el desarrollo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendiendo la información\n",
    "\n",
    "Cualquier uso de Deep Learning requiere el manejo de información. Es decir, vamos a entrenar una red neuronal con training data. Antes de poder saltar a este paso, primero se debe entender cómo es el input. No es lo mismo recibir una imagen a color que una imagen en blanco y negro.\n",
    "\n",
    "Por suerte, Keras ya viene con las utilidades necesarias para descargar MNIST, lo cuál hace más rápido el trabajo. En la siguiente celda descargamos las imágenes y las dividimos en training y testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamos la información\n",
    "Una red neuronal normal no puede recibir matrices como entrada, sólo vectores. Por lo tanto, convertiremos la lista de listas (matriz) en una lista completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01176471,  0.07058824,  0.07058824,\n",
       "        0.07058824,  0.49411765,  0.53333336,  0.68627453,  0.10196079,\n",
       "        0.65098041,  1.        ,  0.96862745,  0.49803922,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.11764706,  0.14117648,  0.36862746,  0.60392159,\n",
       "        0.66666669,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.88235295,  0.67450982,  0.99215686,  0.94901961,\n",
       "        0.7647059 ,  0.25098041,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.19215687,  0.93333334,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.98431373,  0.36470589,\n",
       "        0.32156864,  0.32156864,  0.21960784,  0.15294118,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.85882354,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.7764706 ,  0.71372551,\n",
       "        0.96862745,  0.94509804,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.3137255 ,  0.61176473,  0.41960785,  0.99215686,  0.99215686,\n",
       "        0.80392158,  0.04313726,  0.        ,  0.16862746,  0.60392159,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.05490196,\n",
       "        0.00392157,  0.60392159,  0.99215686,  0.35294119,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54509807,\n",
       "        0.99215686,  0.74509805,  0.00784314,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.04313726,  0.74509805,  0.99215686,\n",
       "        0.27450982,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.13725491,  0.94509804,  0.88235295,  0.627451  ,\n",
       "        0.42352942,  0.00392157,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.31764707,  0.94117647,  0.99215686,  0.99215686,  0.46666667,\n",
       "        0.09803922,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.17647059,\n",
       "        0.72941178,  0.99215686,  0.99215686,  0.58823532,  0.10588235,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.0627451 ,  0.36470589,\n",
       "        0.98823529,  0.99215686,  0.73333335,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.97647059,  0.99215686,\n",
       "        0.97647059,  0.25098041,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.18039216,  0.50980395,\n",
       "        0.71764708,  0.99215686,  0.99215686,  0.81176472,  0.00784314,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.15294118,\n",
       "        0.58039218,  0.89803922,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.98039216,  0.71372551,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.09411765,  0.44705883,  0.86666667,  0.99215686,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.78823531,  0.30588236,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09019608,  0.25882354,  0.83529413,  0.99215686,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.7764706 ,  0.31764707,\n",
       "        0.00784314,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.07058824,  0.67058825,  0.85882354,\n",
       "        0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.7647059 ,\n",
       "        0.3137255 ,  0.03529412,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.21568628,  0.67450982,\n",
       "        0.88627452,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.95686275,  0.52156866,  0.04313726,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.53333336,  0.99215686,  0.99215686,  0.99215686,\n",
       "        0.83137256,  0.52941179,  0.51764709,  0.0627451 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ccd5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_digit(index):\n",
    "    label = y_train[index].argmax(axis=0)\n",
    "    image = x_train[index].reshape([28,28])\n",
    "    plt.title('Training data. Label: {}'.format(label))\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.show()\n",
    "show_digit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A los vectores les aplicamos [one hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f), una técnica importante para que las redes neuronales puedan utilizarlo. \n",
    "\n",
    "Usamos https://keras.io/utils/#to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Has el one hot encoding para que 5 se convierta en [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro valor de X, tenemos la entrada, la cuál es la representación de la imagen como vector. En nuestro valor de Y, tenemos el dígito (0 a 9). Dado que manejamos one hot encoding, lo representamos como un vector de 10 elementos. Es importante tener esto en cuenta cuando diseñemos nuestra red neuronal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construimos arquitectura de red neuronal\n",
    "\n",
    "Para el entrenamiento utilizaremos 60,000 imágenes. Para hacer el testing utilizaremos 10,000 imágenes. ¿Cuál es la diferencia?\n",
    "\n",
    "El proceso de entrenamiento es el siguiente para cada imagen.\n",
    "\n",
    "* La imagen pasa por la red neuronal. (Feedforward)\n",
    "* La red neuronal predice qué número es.\n",
    "* Como estamos en entrenamiento, tenemos el valor de Y (el dígito verdadero).\n",
    "* En base a esto, utilizando un proceso llamado backpropagation, la red neuronal adaptará sus pesos. Usamos Gradient Descent en este paso.\n",
    "* Este proceso lo repetiremos para las 60,000 imágenes. Todo esto ocurre en un epoch. Dependiendo el tipo de problema, queremos que todo nuestro testing set pase varias veces. Para los problemas sencillos, entre 10 y 200 epochs suele ser más que suficiente.\n",
    "\n",
    "* Así que, si tenemos 60,000 imágenes y queremos entrenar nuestra red neuronal en 10 epochs, estaremos hablando de pasar 600,000 imágenes a la red neuronal. Con razón recién es un área que ha empezado a ganar popularidad en los últimos años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 104,938\n",
      "Trainable params: 104,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 1.3659 - acc: 0.6359 - val_loss: 0.6606 - val_acc: 0.8401\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.5315 - acc: 0.8601 - val_loss: 0.4242 - val_acc: 0.8856\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.4034 - acc: 0.8875 - val_loss: 0.3566 - val_acc: 0.8985\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.3526 - acc: 0.9000 - val_loss: 0.3216 - val_acc: 0.9089\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.3224 - acc: 0.9074 - val_loss: 0.2999 - val_acc: 0.9133\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.3004 - acc: 0.9132 - val_loss: 0.2807 - val_acc: 0.9194\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.2829 - acc: 0.9183 - val_loss: 0.2674 - val_acc: 0.9225\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.2684 - acc: 0.9230 - val_loss: 0.2529 - val_acc: 0.9279\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.2553 - acc: 0.9270 - val_loss: 0.2435 - val_acc: 0.9302\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2437 - acc: 0.9304 - val_loss: 0.2341 - val_acc: 0.9333\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2337 - acc: 0.9336 - val_loss: 0.2248 - val_acc: 0.9349\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2237 - acc: 0.9362 - val_loss: 0.2161 - val_acc: 0.9380\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2152 - acc: 0.9388 - val_loss: 0.2095 - val_acc: 0.9399\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.2073 - acc: 0.9411 - val_loss: 0.2015 - val_acc: 0.9409\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1996 - acc: 0.9436 - val_loss: 0.1955 - val_acc: 0.9436\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.1928 - acc: 0.9449 - val_loss: 0.1916 - val_acc: 0.9447\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1864 - acc: 0.9474 - val_loss: 0.1845 - val_acc: 0.9452\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1801 - acc: 0.9486 - val_loss: 0.1793 - val_acc: 0.9466\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.1746 - acc: 0.9503 - val_loss: 0.1757 - val_acc: 0.9484\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.1691 - acc: 0.9521 - val_loss: 0.1720 - val_acc: 0.9497\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1641 - acc: 0.9531 - val_loss: 0.1665 - val_acc: 0.9510\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 4s 58us/step - loss: 0.1593 - acc: 0.9547 - val_loss: 0.1634 - val_acc: 0.9514\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.1551 - acc: 0.9561 - val_loss: 0.1584 - val_acc: 0.9527\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1508 - acc: 0.9571 - val_loss: 0.1561 - val_acc: 0.9534\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.1468 - acc: 0.9579 - val_loss: 0.1519 - val_acc: 0.9536\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.1430 - acc: 0.9592 - val_loss: 0.1506 - val_acc: 0.9551\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1395 - acc: 0.9602 - val_loss: 0.1469 - val_acc: 0.9560\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1360 - acc: 0.9613 - val_loss: 0.1430 - val_acc: 0.9568\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1328 - acc: 0.9622 - val_loss: 0.1398 - val_acc: 0.9576\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.1298 - acc: 0.9633 - val_loss: 0.1382 - val_acc: 0.9585\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1267 - acc: 0.9637 - val_loss: 0.1359 - val_acc: 0.9592\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1238 - acc: 0.9652 - val_loss: 0.1327 - val_acc: 0.9588\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1210 - acc: 0.9658 - val_loss: 0.1303 - val_acc: 0.9608\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1186 - acc: 0.9667 - val_loss: 0.1282 - val_acc: 0.9608\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.1159 - acc: 0.9674 - val_loss: 0.1259 - val_acc: 0.9622\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.1135 - acc: 0.9675 - val_loss: 0.1249 - val_acc: 0.9620\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1111 - acc: 0.9690 - val_loss: 0.1220 - val_acc: 0.9629\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.1088 - acc: 0.9694 - val_loss: 0.1215 - val_acc: 0.9642\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.1065 - acc: 0.9698 - val_loss: 0.1183 - val_acc: 0.9640\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1044 - acc: 0.9708 - val_loss: 0.1187 - val_acc: 0.9636\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1024 - acc: 0.9712 - val_loss: 0.1159 - val_acc: 0.9652\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1005 - acc: 0.9720 - val_loss: 0.1152 - val_acc: 0.9649\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0986 - acc: 0.9725 - val_loss: 0.1158 - val_acc: 0.9658\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0966 - acc: 0.9729 - val_loss: 0.1129 - val_acc: 0.9661\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0950 - acc: 0.9734 - val_loss: 0.1107 - val_acc: 0.9673\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0929 - acc: 0.9737 - val_loss: 0.1132 - val_acc: 0.9665\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0913 - acc: 0.9746 - val_loss: 0.1086 - val_acc: 0.9679\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0896 - acc: 0.9747 - val_loss: 0.1085 - val_acc: 0.9682\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0881 - acc: 0.9753 - val_loss: 0.1073 - val_acc: 0.9676\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0864 - acc: 0.9760 - val_loss: 0.1043 - val_acc: 0.9683\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0850 - acc: 0.9764 - val_loss: 0.1041 - val_acc: 0.9683\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0834 - acc: 0.9770 - val_loss: 0.1029 - val_acc: 0.9696\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0821 - acc: 0.9773 - val_loss: 0.1014 - val_acc: 0.9691\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0806 - acc: 0.9782 - val_loss: 0.0995 - val_acc: 0.9697\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0794 - acc: 0.9782 - val_loss: 0.0995 - val_acc: 0.9703\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0780 - acc: 0.9785 - val_loss: 0.0983 - val_acc: 0.9705\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0767 - acc: 0.9788 - val_loss: 0.0975 - val_acc: 0.9705\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0756 - acc: 0.9792 - val_loss: 0.0971 - val_acc: 0.9707\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0742 - acc: 0.9797 - val_loss: 0.0961 - val_acc: 0.9704\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0730 - acc: 0.9800 - val_loss: 0.0971 - val_acc: 0.9714\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0718 - acc: 0.9805 - val_loss: 0.0954 - val_acc: 0.9708\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0707 - acc: 0.9810 - val_loss: 0.0938 - val_acc: 0.9712\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0695 - acc: 0.9811 - val_loss: 0.0926 - val_acc: 0.9719\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0685 - acc: 0.9815 - val_loss: 0.0923 - val_acc: 0.9716\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0675 - acc: 0.9816 - val_loss: 0.0921 - val_acc: 0.9716\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0665 - acc: 0.9821 - val_loss: 0.0916 - val_acc: 0.9716\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0653 - acc: 0.9825 - val_loss: 0.0910 - val_acc: 0.9725\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0643 - acc: 0.9828 - val_loss: 0.0919 - val_acc: 0.9719\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0635 - acc: 0.9828 - val_loss: 0.0895 - val_acc: 0.9723\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0625 - acc: 0.9831 - val_loss: 0.0911 - val_acc: 0.9724\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0617 - acc: 0.9833 - val_loss: 0.0896 - val_acc: 0.9730\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0607 - acc: 0.9835 - val_loss: 0.0886 - val_acc: 0.9732\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0599 - acc: 0.9838 - val_loss: 0.0888 - val_acc: 0.9726\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0590 - acc: 0.9846 - val_loss: 0.0881 - val_acc: 0.9731\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0582 - acc: 0.9845 - val_loss: 0.0865 - val_acc: 0.9731\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0574 - acc: 0.9847 - val_loss: 0.0869 - val_acc: 0.9741\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0565 - acc: 0.9848 - val_loss: 0.0870 - val_acc: 0.9736\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0557 - acc: 0.9852 - val_loss: 0.0874 - val_acc: 0.9734\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0550 - acc: 0.9855 - val_loss: 0.0868 - val_acc: 0.9736\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0542 - acc: 0.9859 - val_loss: 0.0856 - val_acc: 0.9737\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0535 - acc: 0.9857 - val_loss: 0.0864 - val_acc: 0.9731\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0527 - acc: 0.9862 - val_loss: 0.0848 - val_acc: 0.9742\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0521 - acc: 0.9864 - val_loss: 0.0855 - val_acc: 0.9734\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0513 - acc: 0.9864 - val_loss: 0.0844 - val_acc: 0.9742\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0506 - acc: 0.9868 - val_loss: 0.0846 - val_acc: 0.9747\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0500 - acc: 0.9870 - val_loss: 0.0839 - val_acc: 0.9739\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0492 - acc: 0.9870 - val_loss: 0.0838 - val_acc: 0.9753\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0487 - acc: 0.9875 - val_loss: 0.0844 - val_acc: 0.9743\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0480 - acc: 0.9876 - val_loss: 0.0824 - val_acc: 0.9753\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0473 - acc: 0.9878 - val_loss: 0.0823 - val_acc: 0.9754\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0468 - acc: 0.9882 - val_loss: 0.0826 - val_acc: 0.9746\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0460 - acc: 0.9885 - val_loss: 0.0821 - val_acc: 0.9757\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0455 - acc: 0.9885 - val_loss: 0.0816 - val_acc: 0.9757\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.0815 - val_acc: 0.9751\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0443 - acc: 0.9888 - val_loss: 0.0808 - val_acc: 0.9755\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0438 - acc: 0.9891 - val_loss: 0.0813 - val_acc: 0.9755\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0431 - acc: 0.9891 - val_loss: 0.0811 - val_acc: 0.9761\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0428 - acc: 0.9892 - val_loss: 0.0816 - val_acc: 0.9755\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0420 - acc: 0.9895 - val_loss: 0.0813 - val_acc: 0.9753\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0416 - acc: 0.9895 - val_loss: 0.0803 - val_acc: 0.9763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ddfc080>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9763\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
